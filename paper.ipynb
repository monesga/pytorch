{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+\n",
    "try:\n",
    "    import torch\n",
    "    import torchvision\n",
    "    assert int(torch.__version__.split(\".\")[1]) >= 12, \"torch version should be 1.12+\"\n",
    "    assert int(torchvision.__version__.split(\".\")[1]) >= 13, \"torchvision version should be 0.13+\"\n",
    "    print(f\"torch version: {torch.__version__}\")\n",
    "    print(f\"torchvision version: {torchvision.__version__}\")\n",
    "except:\n",
    "    print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n",
    "    !pip3 install -U torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n",
    "    import torch\n",
    "    import torchvision\n",
    "    print(f\"torch version: {torch.__version__}\")\n",
    "    print(f\"torchvision version: {torchvision.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue with regular imports\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "\n",
    "# Try to get torchinfo, install it if it doesn't work\n",
    "try:\n",
    "    from torchinfo import summary\n",
    "except:\n",
    "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
    "    !pip install -q torchinfo\n",
    "    from torchinfo import summary\n",
    "\n",
    "# Try to import the going_modular directory, download it from GitHub if it doesn't work\n",
    "try:\n",
    "    from modules import data_setup, engine\n",
    "    from helper_functions import download_data, set_seeds, plot_loss_curves\n",
    "except:\n",
    "    # Get the going_modular scripts\n",
    "    print(\"[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download pizza, steak, sushi images from GitHub\n",
    "image_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n",
    "                           destination=\"pizza_steak_sushi\")\n",
    "image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup directory paths to train and test images\n",
    "train_dir = image_path / \"train\"\n",
    "test_dir = image_path / \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create image size (from Table 3 in the ViT paper) \n",
    "IMG_SIZE = 224\n",
    "\n",
    "# Create transform pipeline manually\n",
    "manual_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "])           \n",
    "print(f\"Manually created transforms: {manual_transforms}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the batch size\n",
    "BATCH_SIZE = 32 # this is lower than the ViT paper but it's because we're starting small\n",
    "\n",
    "# Create data loaders\n",
    "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n",
    "    train_dir=train_dir,\n",
    "    test_dir=test_dir,\n",
    "    transform=manual_transforms, # use manually created transforms\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "train_dataloader, test_dataloader, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of images\n",
    "image_batch, label_batch = next(iter(train_dataloader))\n",
    "\n",
    "# Get a single image from the batch\n",
    "image, label = image_batch[0], label_batch[0]\n",
    "\n",
    "# View the batch shapes\n",
    "image.shape, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot image with matplotlib\n",
    "plt.imshow(image.permute(1, 2, 0)) # rearrange image dimensions to suit matplotlib [color_channels, height, width] -> [height, width, color_channels]\n",
    "plt.title(class_names[label])\n",
    "plt.axis(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create example values\n",
    "height = 224 # H (\"The training resolution is 224.\")\n",
    "width = 224 # W\n",
    "color_channels = 3 # C\n",
    "patch_size = 16 # P\n",
    "\n",
    "# Calculate N (number of patches)\n",
    "number_of_patches = int((height * width) / patch_size**2)\n",
    "print(f\"Number of patches (N) with image height (H={height}), width (W={width}) and patch size (P={patch_size}): {number_of_patches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input shape (this is the size of a single image)\n",
    "embedding_layer_input_shape = (height, width, color_channels)\n",
    "\n",
    "# Output shape\n",
    "embedding_layer_output_shape = (number_of_patches, patch_size**2 * color_channels)\n",
    "\n",
    "print(f\"Input shape (single 2D image): {embedding_layer_input_shape}\")\n",
    "print(f\"Output shape (single 2D image flattened into patches): {embedding_layer_output_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View single image\n",
    "plt.imshow(image.permute(1, 2, 0)) # adjust for matplotlib\n",
    "plt.title(class_names[label])\n",
    "plt.axis(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change image shape to be compatible with matplotlib (color_channels, height, width) -> (height, width, color_channels) \n",
    "image_permuted = image.permute(1, 2, 0)\n",
    "\n",
    "# Index to plot the top row of patched pixels\n",
    "patch_size = 16\n",
    "plt.figure(figsize=(patch_size, patch_size))\n",
    "plt.imshow(image_permuted[:patch_size, :, :]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup hyperparameters and make sure img_size and patch_size are compatible\n",
    "img_size = 224\n",
    "patch_size = 16\n",
    "num_patches = img_size/patch_size \n",
    "assert img_size % patch_size == 0, \"Image size must be divisible by patch size\" \n",
    "print(f\"Number of patches per row: {num_patches}\\nPatch size: {patch_size} pixels x {patch_size} pixels\")\n",
    "\n",
    "# Create a series of subplots\n",
    "fig, axs = plt.subplots(nrows=1, \n",
    "                        ncols=img_size // patch_size, # one column for each patch\n",
    "                        figsize=(num_patches, num_patches),\n",
    "                        sharex=True,\n",
    "                        sharey=True)\n",
    "\n",
    "# Iterate through number of patches in the top row\n",
    "for i, patch in enumerate(range(0, img_size, patch_size)):\n",
    "    axs[i].imshow(image_permuted[:patch_size, patch:patch+patch_size, :]); # keep height index constant, alter the width index\n",
    "    axs[i].set_xlabel(i+1) # set the label\n",
    "    axs[i].set_xticks([])\n",
    "    axs[i].set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup hyperparameters and make sure img_size and patch_size are compatible\n",
    "img_size = 224\n",
    "patch_size = 16\n",
    "num_patches = img_size/patch_size \n",
    "assert img_size % patch_size == 0, \"Image size must be divisible by patch size\" \n",
    "print(f\"Number of patches per row: {num_patches}\\\n",
    "        \\nNumber of patches per column: {num_patches}\\\n",
    "        \\nTotal patches: {num_patches*num_patches}\\\n",
    "        \\nPatch size: {patch_size} pixels x {patch_size} pixels\")\n",
    "\n",
    "# Create a series of subplots\n",
    "fig, axs = plt.subplots(nrows=img_size // patch_size, # need int not float\n",
    "                        ncols=img_size // patch_size, \n",
    "                        figsize=(num_patches, num_patches),\n",
    "                        sharex=True,\n",
    "                        sharey=True)\n",
    "\n",
    "# Loop through height and width of image\n",
    "for i, patch_height in enumerate(range(0, img_size, patch_size)): # iterate through height\n",
    "    for j, patch_width in enumerate(range(0, img_size, patch_size)): # iterate through width\n",
    "        \n",
    "        # Plot the permuted image patch (image_permuted -> (Height, Width, Color Channels))\n",
    "        axs[i, j].imshow(image_permuted[patch_height:patch_height+patch_size, # iterate through height \n",
    "                                        patch_width:patch_width+patch_size, # iterate through width\n",
    "                                        :]) # get all color channels\n",
    "        \n",
    "        # Set up label information, remove the ticks for clarity and set labels to outside\n",
    "        axs[i, j].set_ylabel(i+1, \n",
    "                             rotation=\"horizontal\", \n",
    "                             horizontalalignment=\"right\", \n",
    "                             verticalalignment=\"center\") \n",
    "        axs[i, j].set_xlabel(j+1) \n",
    "        axs[i, j].set_xticks([])\n",
    "        axs[i, j].set_yticks([])\n",
    "        axs[i, j].label_outer()\n",
    "\n",
    "# Set a super title\n",
    "fig.suptitle(f\"{class_names[label]} -> Patchified\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "# Set the patch size\n",
    "patch_size=16\n",
    "\n",
    "# Create the Conv2d layer with hyperparameters from the ViT paper\n",
    "conv2d = nn.Conv2d(in_channels=3, # number of color channels\n",
    "                   out_channels=768, # from Table 1: Hidden size D, this is the embedding size\n",
    "                   kernel_size=patch_size, # could also use (patch_size, patch_size)\n",
    "                   stride=patch_size,\n",
    "                   padding=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View single image\n",
    "plt.imshow(image.permute(1, 2, 0)) # adjust for matplotlib\n",
    "plt.title(class_names[label])\n",
    "plt.axis(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the image through the convolutional layer \n",
    "image_out_of_conv = conv2d(image.unsqueeze(0)) # add a single batch dimension (height, width, color_channels) -> (batch, height, width, color_channels)\n",
    "print(image_out_of_conv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot random 5 convolutional feature maps\n",
    "import random\n",
    "random_indexes = random.sample(range(0, 758), k=5) # pick 5 numbers between 0 and the embedding size\n",
    "print(f\"Showing random convolutional feature maps from indexes: {random_indexes}\")\n",
    "\n",
    "# Create plot\n",
    "fig, axs = plt.subplots(nrows=1, ncols=5, figsize=(12, 12))\n",
    "\n",
    "# Plot random image feature maps\n",
    "for i, idx in enumerate(random_indexes):\n",
    "    image_conv_feature_map = image_out_of_conv[:, idx, :, :] # index on the output tensor of the convolutional layer\n",
    "    axs[i].imshow(image_conv_feature_map.squeeze().detach().numpy())\n",
    "    axs[i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a single feature map in tensor form\n",
    "single_feature_map = image_out_of_conv[:, 0, :, :]\n",
    "single_feature_map, single_feature_map.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current tensor shape\n",
    "print(f\"Current tensor shape: {image_out_of_conv.shape} -> [batch, embedding_dim, feature_map_height, feature_map_width]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create flatten layer\n",
    "flatten = nn.Flatten(start_dim=2, # flatten feature_map_height (dimension 2)\n",
    "                     end_dim=3) # flatten feature_map_width (dimension 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. View single image\n",
    "plt.imshow(image.permute(1, 2, 0)) # adjust for matplotlib\n",
    "plt.title(class_names[label])\n",
    "plt.axis(False);\n",
    "print(f\"Original image shape: {image.shape}\")\n",
    "\n",
    "# 2. Turn image into feature maps\n",
    "image_out_of_conv = conv2d(image.unsqueeze(0)) # add batch dimension to avoid shape errors\n",
    "print(f\"Image feature map shape: {image_out_of_conv.shape}\")\n",
    "\n",
    "# 3. Flatten the feature maps\n",
    "image_out_of_conv_flattened = flatten(image_out_of_conv)\n",
    "print(f\"Flattened image feature map shape: {image_out_of_conv_flattened.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get flattened image patch embeddings in right shape \n",
    "image_out_of_conv_flattened_reshaped = image_out_of_conv_flattened.permute(0, 2, 1) # [batch_size, P^2•C, N] -> [batch_size, N, P^2•C]\n",
    "print(f\"Patch embedding sequence shape: {image_out_of_conv_flattened_reshaped.shape} -> [batch_size, num_patches, embedding_size]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a single flattened feature map\n",
    "single_flattened_feature_map = image_out_of_conv_flattened_reshaped[:, :, 0] # index: (batch_size, number_of_patches, embedding_dimension)\n",
    "\n",
    "# Plot the flattened feature map visually\n",
    "plt.figure(figsize=(22, 22))\n",
    "plt.imshow(single_flattened_feature_map.detach().numpy())\n",
    "plt.title(f\"Flattened feature map shape: {single_flattened_feature_map.shape}\")\n",
    "plt.axis(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the flattened feature map as a tensor\n",
    "single_flattened_feature_map, single_flattened_feature_map.requires_grad, single_flattened_feature_map.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a class which subclasses nn.Module\n",
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"Turns a 2D input image into a 1D sequence learnable embedding vector.\n",
    "    \n",
    "    Args:\n",
    "        in_channels (int): Number of color channels for the input images. Defaults to 3.\n",
    "        patch_size (int): Size of patches to convert input image into. Defaults to 16.\n",
    "        embedding_dim (int): Size of embedding to turn image into. Defaults to 768.\n",
    "    \"\"\" \n",
    "    # 2. Initialize the class with appropriate variables\n",
    "    def __init__(self, \n",
    "                 in_channels:int=3,\n",
    "                 patch_size:int=16,\n",
    "                 embedding_dim:int=768):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 3. Create a layer to turn an image into patches\n",
    "        self.patcher = nn.Conv2d(in_channels=in_channels,\n",
    "                                 out_channels=embedding_dim,\n",
    "                                 kernel_size=patch_size,\n",
    "                                 stride=patch_size,\n",
    "                                 padding=0)\n",
    "\n",
    "        # 4. Create a layer to flatten the patch feature maps into a single dimension\n",
    "        self.flatten = nn.Flatten(start_dim=2, # only flatten the feature map dimensions into a single vector\n",
    "                                  end_dim=3)\n",
    "\n",
    "    # 5. Define the forward method \n",
    "    def forward(self, x):\n",
    "        # Create assertion to check that inputs are the correct shape\n",
    "        image_resolution = x.shape[-1]\n",
    "        assert image_resolution % patch_size == 0, f\"Input image size must be divisble by patch size, image shape: {image_resolution}, patch size: {patch_size}\"\n",
    "        \n",
    "        # Perform the forward pass\n",
    "        x_patched = self.patcher(x)\n",
    "        x_flattened = self.flatten(x_patched) \n",
    "        # 6. Make sure the output shape has the right order \n",
    "        return x_flattened.permute(0, 2, 1) # adjust so the embedding is on the final dimension [batch_size, P^2•C, N] -> [batch_size, N, P^2•C]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds()\n",
    "\n",
    "# Create an instance of patch embedding layer\n",
    "patchify = PatchEmbedding(in_channels=3,\n",
    "                          patch_size=16,\n",
    "                          embedding_dim=768)\n",
    "\n",
    "# Pass a single image through\n",
    "print(f\"Input image shape: {image.unsqueeze(0).shape}\")\n",
    "patch_embedded_image = patchify(image.unsqueeze(0)) # add an extra batch dimension on the 0th index, otherwise will error\n",
    "print(f\"Output patch embedding shape: {patch_embedded_image.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random input sizes\n",
    "random_input_image = (1, 3, 224, 224)\n",
    "random_input_image_error = (1, 3, 250, 250) # will error because image size is incompatible with patch_size\n",
    "\n",
    "# # Get a summary of the input and outputs of PatchEmbedding (uncomment for full output)\n",
    "# summary(PatchEmbedding(), \n",
    "#         input_size=random_input_image, # try swapping this for \"random_input_image_error\" \n",
    "#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "#         col_width=20,\n",
    "#         row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the patch embedding and patch embedding shape\n",
    "print(patch_embedded_image) \n",
    "print(f\"Patch embedding shape: {patch_embedded_image.shape} -> [batch_size, number_of_patches, embedding_dimension]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the batch size and embedding dimension\n",
    "batch_size = patch_embedded_image.shape[0]\n",
    "embedding_dimension = patch_embedded_image.shape[-1]\n",
    "\n",
    "# Create the class token embedding as a learnable parameter that shares the same size as the embedding dimension (D)\n",
    "class_token = nn.Parameter(torch.ones(batch_size, 1, embedding_dimension), # [batch_size, number_of_tokens, embedding_dimension]\n",
    "                           requires_grad=True) # make sure the embedding is learnable\n",
    "\n",
    "# Show the first 10 examples of the class_token\n",
    "print(class_token[:, :, :10])\n",
    "\n",
    "# Print the class_token shape\n",
    "print(f\"Class token shape: {class_token.shape} -> [batch_size, number_of_tokens, embedding_dimension]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the class token embedding to the front of the patch embedding\n",
    "patch_embedded_image_with_class_embedding = torch.cat((class_token, patch_embedded_image), \n",
    "                                                      dim=1) # concat on first dimension\n",
    "\n",
    "# Print the sequence of patch embeddings with the prepended class token embedding\n",
    "print(patch_embedded_image_with_class_embedding)\n",
    "print(f\"Sequence of patch embeddings with class token prepended shape: {patch_embedded_image_with_class_embedding.shape} -> [batch_size, number_of_patches, embedding_dimension]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the sequence of patch embeddings with the prepended class embedding\n",
    "patch_embedded_image_with_class_embedding, patch_embedded_image_with_class_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate N (number of patches)\n",
    "number_of_patches = int((height * width) / patch_size**2)\n",
    "\n",
    "# Get embedding dimension\n",
    "embedding_dimension = patch_embedded_image_with_class_embedding.shape[2]\n",
    "\n",
    "# Create the learnable 1D position embedding\n",
    "position_embedding = nn.Parameter(torch.ones(1,\n",
    "                                             number_of_patches+1, \n",
    "                                             embedding_dimension),\n",
    "                                  requires_grad=True) # make sure it's learnable\n",
    "\n",
    "# Show the first 10 sequences and 10 position embedding values and check the shape of the position embedding\n",
    "print(position_embedding[:, :10, :10])\n",
    "print(f\"Position embeddding shape: {position_embedding.shape} -> [batch_size, number_of_patches, embedding_dimension]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the position embedding to the patch and class token embedding\n",
    "patch_and_position_embedding = patch_embedded_image_with_class_embedding + position_embedding\n",
    "print(patch_and_position_embedding)\n",
    "print(f\"Patch embeddings, class token prepended and positional embeddings added shape: {patch_and_position_embedding.shape} -> [batch_size, number_of_patches, embedding_dimension]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds()\n",
    "\n",
    "# 1. Set patch size\n",
    "patch_size = 16\n",
    "\n",
    "# 2. Print shape of original image tensor and get the image dimensions\n",
    "print(f\"Image tensor shape: {image.shape}\")\n",
    "height, width = image.shape[1], image.shape[2]\n",
    "\n",
    "# 3. Get image tensor and add batch dimension\n",
    "x = image.unsqueeze(0)\n",
    "print(f\"Input image with batch dimension shape: {x.shape}\")\n",
    "\n",
    "# 4. Create patch embedding layer\n",
    "patch_embedding_layer = PatchEmbedding(in_channels=3,\n",
    "                                       patch_size=patch_size,\n",
    "                                       embedding_dim=768)\n",
    "\n",
    "# 5. Pass image through patch embedding layer\n",
    "patch_embedding = patch_embedding_layer(x)\n",
    "print(f\"Patching embedding shape: {patch_embedding.shape}\")\n",
    "\n",
    "# 6. Create class token embedding\n",
    "batch_size = patch_embedding.shape[0]\n",
    "embedding_dimension = patch_embedding.shape[-1]\n",
    "class_token = nn.Parameter(torch.ones(batch_size, 1, embedding_dimension),\n",
    "                           requires_grad=True) # make sure it's learnable\n",
    "print(f\"Class token embedding shape: {class_token.shape}\")\n",
    "\n",
    "# 7. Prepend class token embedding to patch embedding\n",
    "patch_embedding_class_token = torch.cat((class_token, patch_embedding), dim=1)\n",
    "print(f\"Patch embedding with class token shape: {patch_embedding_class_token.shape}\")\n",
    "\n",
    "# 8. Create position embedding\n",
    "number_of_patches = int((height * width) / patch_size**2)\n",
    "position_embedding = nn.Parameter(torch.ones(1, number_of_patches+1, embedding_dimension),\n",
    "                                  requires_grad=True) # make sure it's learnable\n",
    "\n",
    "# 9. Add position embedding to patch embedding with class token\n",
    "patch_and_position_embedding = patch_embedding_class_token + position_embedding\n",
    "print(f\"Patch and position embedding shape: {patch_and_position_embedding.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a class that inherits from nn.Module\n",
    "class MultiheadSelfAttentionBlock(nn.Module):\n",
    "    \"\"\"Creates a multi-head self-attention block (\"MSA block\" for short).\n",
    "    \"\"\"\n",
    "    # 2. Initialize the class with hyperparameters from Table 1\n",
    "    def __init__(self,\n",
    "                 embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base\n",
    "                 num_heads:int=12, # Heads from Table 1 for ViT-Base\n",
    "                 attn_dropout:float=0): # doesn't look like the paper uses any dropout in MSABlocks\n",
    "        super().__init__()\n",
    "        \n",
    "        # 3. Create the Norm layer (LN)\n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
    "        \n",
    "        # 4. Create the Multi-Head Attention (MSA) layer\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim=embedding_dim,\n",
    "                                                    num_heads=num_heads,\n",
    "                                                    dropout=attn_dropout,\n",
    "                                                    batch_first=True) # does our batch dimension come first?\n",
    "        \n",
    "    # 5. Create a forward() method to pass the data throguh the layers\n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm(x)\n",
    "        attn_output, _ = self.multihead_attn(query=x, # query embeddings \n",
    "                                             key=x, # key embeddings\n",
    "                                             value=x, # value embeddings\n",
    "                                             need_weights=False) # do we need the weights or just the layer outputs?\n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of MSABlock\n",
    "multihead_self_attention_block = MultiheadSelfAttentionBlock(embedding_dim=768, # from Table 1 \n",
    "                                                             num_heads=12) # from Table 1\n",
    "\n",
    "# Pass patch and position image embedding through MSABlock\n",
    "patched_image_through_msa_block = multihead_self_attention_block(patch_and_position_embedding)\n",
    "print(f\"Input shape of MSA block: {patch_and_position_embedding.shape}\")\n",
    "print(f\"Output shape MSA block: {patched_image_through_msa_block.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "14a2cfa7e9b6484bab3e614afe1c8d57b8264facec300639b3eb6457044fd274"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
