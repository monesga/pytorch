{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+\n",
    "try:\n",
    "    import torch\n",
    "    import torchvision\n",
    "    assert int(torch.__version__.split(\".\")[1]) >= 12, \"torch version should be 1.12+\"\n",
    "    assert int(torchvision.__version__.split(\".\")[1]) >= 13, \"torchvision version should be 0.13+\"\n",
    "    print(f\"torch version: {torch.__version__}\")\n",
    "    print(f\"torchvision version: {torchvision.__version__}\")\n",
    "except:\n",
    "    print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n",
    "    !pip3 install -U torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n",
    "    import torch\n",
    "    import torchvision\n",
    "    print(f\"torch version: {torch.__version__}\")\n",
    "    print(f\"torchvision version: {torchvision.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue with regular imports\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "\n",
    "# Try to get torchinfo, install it if it doesn't work\n",
    "try:\n",
    "    from torchinfo import summary\n",
    "except:\n",
    "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
    "    !pip install -q torchinfo\n",
    "    from torchinfo import summary\n",
    "\n",
    "# Try to import the going_modular directory, download it from GitHub if it doesn't work\n",
    "try:\n",
    "    from modules import data_setup, engine\n",
    "    from helper_functions import download_data, set_seeds, plot_loss_curves\n",
    "except:\n",
    "    # Get the going_modular scripts\n",
    "    print(\"[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download pizza, steak, sushi images from GitHub\n",
    "data_20_percent_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip\",\n",
    "                                     destination=\"pizza_steak_sushi_20_percent\")\n",
    "\n",
    "data_20_percent_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup directory paths to train and test images\n",
    "train_dir = data_20_percent_path / \"train\"\n",
    "test_dir = data_20_percent_path / \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup pretrained EffNetB2 weights\n",
    "effnetb2_weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
    "\n",
    "# 2. Get EffNetB2 transforms\n",
    "effnetb2_transforms = effnetb2_weights.transforms()\n",
    "\n",
    "# 3. Setup pretrained model\n",
    "effnetb2 = torchvision.models.efficientnet_b2(weights=effnetb2_weights) # could also use weights=\"DEFAULT\"\n",
    "\n",
    "# 4. Freeze the base layers in the model (this will freeze all layers to begin with)\n",
    "for param in effnetb2.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out EffNetB2 classifier head\n",
    "effnetb2.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Update the classifier head\n",
    "effnetb2.classifier = nn.Sequential(\n",
    "    nn.Dropout(p=0.3, inplace=True), # keep dropout layer same\n",
    "    nn.Linear(in_features=1408, # keep in_features same \n",
    "              out_features=3)) # change out_features to suit our number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_effnetb2_model(num_classes:int=3, \n",
    "                          seed:int=42):\n",
    "    \"\"\"Creates an EfficientNetB2 feature extractor model and transforms.\n",
    "\n",
    "    Args:\n",
    "        num_classes (int, optional): number of classes in the classifier head. \n",
    "            Defaults to 3.\n",
    "        seed (int, optional): random seed value. Defaults to 42.\n",
    "\n",
    "    Returns:\n",
    "        model (torch.nn.Module): EffNetB2 feature extractor model. \n",
    "        transforms (torchvision.transforms): EffNetB2 image transforms.\n",
    "    \"\"\"\n",
    "    # 1, 2, 3. Create EffNetB2 pretrained weights, transforms and model\n",
    "    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
    "    transforms = weights.transforms()\n",
    "    model = torchvision.models.efficientnet_b2(weights=weights)\n",
    "\n",
    "    # 4. Freeze all layers in base model\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # 5. Change classifier head with random seed for reproducibility\n",
    "    torch.manual_seed(seed)\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.3, inplace=True),\n",
    "        nn.Linear(in_features=1408, out_features=num_classes),\n",
    "    )\n",
    "    \n",
    "    return model, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effnetb2, effnetb2_transforms = create_effnetb2_model(num_classes=3,\n",
    "                                                      seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# # Print EffNetB2 model summary (uncomment for full output) \n",
    "# summary(effnetb2, \n",
    "#         input_size=(1, 3, 224, 224),\n",
    "#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "#         col_width=20,\n",
    "#         row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup DataLoaders\n",
    "from modules import data_setup\n",
    "train_dataloader_effnetb2, test_dataloader_effnetb2, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
    "                                                                                                 test_dir=test_dir,\n",
    "                                                                                                 transform=effnetb2_transforms,\n",
    "                                                                                                 batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules import engine\n",
    "\n",
    "# Setup optimizer\n",
    "optimizer = torch.optim.Adam(params=effnetb2.parameters(),\n",
    "                             lr=1e-3)\n",
    "# Setup loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Set seeds for reproducibility and train the model\n",
    "set_seeds()\n",
    "effnetb2_results = engine.train(model=effnetb2,\n",
    "                                train_dataloader=train_dataloader_effnetb2,\n",
    "                                test_dataloader=test_dataloader_effnetb2,\n",
    "                                epochs=10,\n",
    "                                optimizer=optimizer,\n",
    "                                loss_fn=loss_fn,\n",
    "                                device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import plot_loss_curves\n",
    "\n",
    "plot_loss_curves(effnetb2_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules import utils\n",
    "\n",
    "# Save the model\n",
    "utils.save_model(model=effnetb2,\n",
    "                 target_dir=\"models\",\n",
    "                 model_name=\"09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Get the model size in bytes then convert to megabytes\n",
    "pretrained_effnetb2_model_size = Path(\"models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\").stat().st_size // (1024*1024) # division converts bytes to megabytes (roughly) \n",
    "print(f\"Pretrained EffNetB2 feature extractor model size: {pretrained_effnetb2_model_size} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of parameters in EffNetB2\n",
    "effnetb2_total_params = sum(torch.numel(param) for param in effnetb2.parameters())\n",
    "effnetb2_total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary with EffNetB2 statistics\n",
    "effnetb2_stats = {\"test_loss\": effnetb2_results[\"test_loss\"][-1],\n",
    "                  \"test_acc\": effnetb2_results[\"test_acc\"][-1],\n",
    "                  \"number_of_parameters\": effnetb2_total_params,\n",
    "                  \"model_size (MB)\": pretrained_effnetb2_model_size}\n",
    "effnetb2_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (head): Linear(in_features=768, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out ViT heads layer\n",
    "vit = torchvision.models.vit_b_16()\n",
    "vit.heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vit_model(num_classes:int=3, \n",
    "                     seed:int=42):\n",
    "    \"\"\"Creates a ViT-B/16 feature extractor model and transforms.\n",
    "\n",
    "    Args:\n",
    "        num_classes (int, optional): number of target classes. Defaults to 3.\n",
    "        seed (int, optional): random seed value for output layer. Defaults to 42.\n",
    "\n",
    "    Returns:\n",
    "        model (torch.nn.Module): ViT-B/16 feature extractor model. \n",
    "        transforms (torchvision.transforms): ViT-B/16 image transforms.\n",
    "    \"\"\"\n",
    "    # Create ViT_B_16 pretrained weights, transforms and model\n",
    "    weights = torchvision.models.ViT_B_16_Weights.DEFAULT\n",
    "    transforms = weights.transforms()\n",
    "    model = torchvision.models.vit_b_16(weights=weights)\n",
    "\n",
    "    # Freeze all layers in model\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Change classifier head to suit our needs (this will be trainable)\n",
    "    torch.manual_seed(seed)\n",
    "    model.heads = nn.Sequential(nn.Linear(in_features=768, # keep this the same as original model\n",
    "                                          out_features=num_classes)) # update to reflect target number of classes\n",
    "    \n",
    "    return model, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ViT model and transforms\n",
    "vit, vit_transforms = create_vit_model(num_classes=3,\n",
    "                                       seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# # Print ViT feature extractor model summary (uncomment for full output)\n",
    "# summary(vit, \n",
    "#         input_size=(1, 3, 224, 224),\n",
    "#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "#         col_width=20,\n",
    "#         row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup ViT DataLoaders\n",
    "from modules import data_setup\n",
    "train_dataloader_vit, test_dataloader_vit, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
    "                                                                                       test_dir=test_dir,\n",
    "                                                                                       transform=vit_transforms,\n",
    "                                                                                       batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules import engine\n",
    "\n",
    "# Setup optimizer\n",
    "optimizer = torch.optim.Adam(params=vit.parameters(),\n",
    "                             lr=1e-3)\n",
    "# Setup loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Train ViT model with seeds set for reproducibility\n",
    "set_seeds()\n",
    "vit_results = engine.train(model=vit,\n",
    "                           train_dataloader=train_dataloader_vit,\n",
    "                           test_dataloader=test_dataloader_vit,\n",
    "                           epochs=10,\n",
    "                           optimizer=optimizer,\n",
    "                           loss_fn=loss_fn,\n",
    "                           device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import plot_loss_curves\n",
    "\n",
    "plot_loss_curves(vit_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saving model to: models\\09_pretrained_vit_feature_extractor_pizza_steak_sushi_20_percent.pth\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "from modules import utils\n",
    "\n",
    "utils.save_model(model=vit,\n",
    "                 target_dir=\"models\",\n",
    "                 model_name=\"09_pretrained_vit_feature_extractor_pizza_steak_sushi_20_percent.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Get the model size in bytes then convert to megabytes\n",
    "pretrained_vit_model_size = Path(\"models/09_pretrained_vit_feature_extractor_pizza_steak_sushi_20_percent.pth\").stat().st_size // (1024*1024) # division converts bytes to megabytes (roughly) \n",
    "print(f\"Pretrained ViT feature extractor model size: {pretrained_vit_model_size} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85800963"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count number of parameters in ViT\n",
    "vit_total_params = sum(torch.numel(param) for param in vit.parameters())\n",
    "vit_total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ViT statistics dictionary\n",
    "vit_stats = {\"test_loss\": vit_results[\"test_loss\"][-1],\n",
    "             \"test_acc\": vit_results[\"test_acc\"][-1],\n",
    "             \"number_of_parameters\": vit_total_params,\n",
    "             \"model_size (MB)\": pretrained_vit_model_size}\n",
    "\n",
    "vit_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Get all test data paths\n",
    "print(f\"[INFO] Finding all filepaths ending with '.jpg' in directory: {test_dir}\")\n",
    "test_data_paths = list(Path(test_dir).glob(\"*/*.jpg\"))\n",
    "test_data_paths[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "from timeit import default_timer as timer \n",
    "from tqdm.auto import tqdm\n",
    "from typing import List, Dict\n",
    "\n",
    "# 1. Create a function to return a list of dictionaries with sample, truth label, prediction, prediction probability and prediction time\n",
    "def pred_and_store(paths: List[pathlib.Path], \n",
    "                   model: torch.nn.Module,\n",
    "                   transform: torchvision.transforms, \n",
    "                   class_names: List[str], \n",
    "                   device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\") -> List[Dict]:\n",
    "    \n",
    "    # 2. Create an empty list to store prediction dictionaires\n",
    "    pred_list = []\n",
    "    \n",
    "    # 3. Loop through target paths\n",
    "    for path in tqdm(paths):\n",
    "        \n",
    "        # 4. Create empty dictionary to store prediction information for each sample\n",
    "        pred_dict = {}\n",
    "\n",
    "        # 5. Get the sample path and ground truth class name\n",
    "        pred_dict[\"image_path\"] = path\n",
    "        class_name = path.parent.stem\n",
    "        pred_dict[\"class_name\"] = class_name\n",
    "        \n",
    "        # 6. Start the prediction timer\n",
    "        start_time = timer()\n",
    "        \n",
    "        # 7. Open image path\n",
    "        img = Image.open(path)\n",
    "        \n",
    "        # 8. Transform the image, add batch dimension and put image on target device\n",
    "        transformed_image = transform(img).unsqueeze(0).to(device) \n",
    "        \n",
    "        # 9. Prepare model for inference by sending it to target device and turning on eval() mode\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        \n",
    "        # 10. Get prediction probability, predicition label and prediction class\n",
    "        with torch.inference_mode():\n",
    "            pred_logit = model(transformed_image) # perform inference on target sample \n",
    "            pred_prob = torch.softmax(pred_logit, dim=1) # turn logits into prediction probabilities\n",
    "            pred_label = torch.argmax(pred_prob, dim=1) # turn prediction probabilities into prediction label\n",
    "            pred_class = class_names[pred_label.cpu()] # hardcode prediction class to be on CPU\n",
    "\n",
    "            # 11. Make sure things in the dictionary are on CPU (required for inspecting predictions later on) \n",
    "            pred_dict[\"pred_prob\"] = round(pred_prob.unsqueeze(0).max().cpu().item(), 4)\n",
    "            pred_dict[\"pred_class\"] = pred_class\n",
    "            \n",
    "            # 12. End the timer and calculate time per pred\n",
    "            end_time = timer()\n",
    "            pred_dict[\"time_for_pred\"] = round(end_time-start_time, 4)\n",
    "\n",
    "        # 13. Does the pred match the true label?\n",
    "        pred_dict[\"correct\"] = class_name == pred_class\n",
    "\n",
    "        # 14. Add the dictionary to the list of preds\n",
    "        pred_list.append(pred_dict)\n",
    "    \n",
    "    # 15. Return list of prediction dictionaries\n",
    "    return pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [00:13<00:00, 11.50it/s]\n"
     ]
    }
   ],
   "source": [
    "# Make predictions across test dataset with EffNetB2\n",
    "effnetb2_test_pred_dicts = pred_and_store(paths=test_data_paths,\n",
    "                                          model=effnetb2,\n",
    "                                          transform=effnetb2_transforms,\n",
    "                                          class_names=class_names,\n",
    "                                          device=\"cpu\") # make predictions on CPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the first 2 prediction dictionaries\n",
    "effnetb2_test_pred_dicts[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>class_name</th>\n",
       "      <th>pred_prob</th>\n",
       "      <th>pred_class</th>\n",
       "      <th>time_for_pred</th>\n",
       "      <th>correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data\\pizza_steak_sushi_20_percent\\test\\pizza\\1...</td>\n",
       "      <td>pizza</td>\n",
       "      <td>0.9903</td>\n",
       "      <td>pizza</td>\n",
       "      <td>0.1038</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data\\pizza_steak_sushi_20_percent\\test\\pizza\\1...</td>\n",
       "      <td>pizza</td>\n",
       "      <td>0.4962</td>\n",
       "      <td>pizza</td>\n",
       "      <td>0.0846</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data\\pizza_steak_sushi_20_percent\\test\\pizza\\1...</td>\n",
       "      <td>pizza</td>\n",
       "      <td>0.9924</td>\n",
       "      <td>pizza</td>\n",
       "      <td>0.0836</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data\\pizza_steak_sushi_20_percent\\test\\pizza\\1...</td>\n",
       "      <td>pizza</td>\n",
       "      <td>0.6767</td>\n",
       "      <td>pizza</td>\n",
       "      <td>0.0896</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data\\pizza_steak_sushi_20_percent\\test\\pizza\\1...</td>\n",
       "      <td>pizza</td>\n",
       "      <td>0.7553</td>\n",
       "      <td>pizza</td>\n",
       "      <td>0.0836</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          image_path class_name  pred_prob  \\\n",
       "0  data\\pizza_steak_sushi_20_percent\\test\\pizza\\1...      pizza     0.9903   \n",
       "1  data\\pizza_steak_sushi_20_percent\\test\\pizza\\1...      pizza     0.4962   \n",
       "2  data\\pizza_steak_sushi_20_percent\\test\\pizza\\1...      pizza     0.9924   \n",
       "3  data\\pizza_steak_sushi_20_percent\\test\\pizza\\1...      pizza     0.6767   \n",
       "4  data\\pizza_steak_sushi_20_percent\\test\\pizza\\1...      pizza     0.7553   \n",
       "\n",
       "  pred_class  time_for_pred  correct  \n",
       "0      pizza         0.1038     True  \n",
       "1      pizza         0.0846     True  \n",
       "2      pizza         0.0836     True  \n",
       "3      pizza         0.0896     True  \n",
       "4      pizza         0.0836     True  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn the test_pred_dicts into a DataFrame\n",
    "import pandas as pd\n",
    "effnetb2_test_pred_df = pd.DataFrame(effnetb2_test_pred_dicts)\n",
    "effnetb2_test_pred_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "14a2cfa7e9b6484bab3e614afe1c8d57b8264facec300639b3eb6457044fd274"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
